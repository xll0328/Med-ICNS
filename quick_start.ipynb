{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_explain as te\n",
    "from torch_explain import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, c, y = datasets.xor(500)\n",
    "x_train, x_test, c_train, c_test, y_train, y_test = train_test_split(x, c, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_explain as te\n",
    "\n",
    "embedding_size = 8\n",
    "concept_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x.shape[1], 10),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    te.nn.ConceptEmbedding(10, c.shape[1], embedding_size),\n",
    ")\n",
    "task_predictor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(c.shape[1]*embedding_size, 1),\n",
    ")\n",
    "model = torch.nn.Sequential(concept_encoder, task_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_explain as te\n",
    "\n",
    "embedding_size = 8\n",
    "concept_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(x.shape[1], 10),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    te.nn.ConceptEmbedding(10, c.shape[1], embedding_size),\n",
    ")\n",
    "task_predictor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(c.shape[1]*embedding_size, 1),\n",
    ")\n",
    "model = torch.nn.Sequential(concept_encoder, task_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x102 and 2x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\project\\XAI4Medical\\pytorch_explain\\quick_start.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/project/XAI4Medical/pytorch_explain/quick_start.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/project/XAI4Medical/pytorch_explain/quick_start.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# generate concept and task predictions\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/project/XAI4Medical/pytorch_explain/quick_start.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m c_emb, c_pred \u001b[39m=\u001b[39m concept_encoder(x_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/XAI4Medical/pytorch_explain/quick_start.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_pred \u001b[39m=\u001b[39m task_predictor(c_emb\u001b[39m.\u001b[39mreshape(\u001b[39mlen\u001b[39m(c_emb), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/project/XAI4Medical/pytorch_explain/quick_start.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# compute loss\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\dcr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\dcr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\dcr\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\dcr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\dcr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\dcr\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x102 and 2x10)"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_form_c = torch.nn.BCELoss()\n",
    "loss_form_y = torch.nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for epoch in range(501):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # generate concept and task predictions\n",
    "    c_emb, c_pred = concept_encoder(x_train)\n",
    "    y_pred = task_predictor(c_emb.reshape(len(c_emb), -1))\n",
    "\n",
    "    # compute loss\n",
    "    concept_loss = loss_form_c(c_pred, c_train)\n",
    "    task_loss = loss_form_y(y_pred, y_train)\n",
    "    loss = concept_loss + 0.5*task_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_emb, c_pred = concept_encoder.forward(x_test)\n",
    "y_pred = task_predictor(c_emb.reshape(len(c_emb), -1))\n",
    "\n",
    "task_accuracy = accuracy_score(y_test, y_pred > 0)\n",
    "concept_accuracy = accuracy_score(c_test, c_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_explain.nn.concepts import ConceptReasoningLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y_train = F.one_hot(y_train.long().ravel()).float()\n",
    "y_test = F.one_hot(y_test.long().ravel()).float()\n",
    "\n",
    "task_predictor = ConceptReasoningLayer(embedding_size, y_train.shape[1])\n",
    "model = torch.nn.Sequential(concept_encoder, task_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_form = torch.nn.BCELoss()\n",
    "model.train()\n",
    "for epoch in range(501):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # generate concept and task predictions\n",
    "    c_emb, c_pred = concept_encoder(x_train)\n",
    "    y_pred = task_predictor(c_emb, c_pred)\n",
    "\n",
    "    # compute loss\n",
    "    concept_loss = loss_form(c_pred, c_train)\n",
    "    task_loss = loss_form(y_pred, y_train)\n",
    "    loss = concept_loss + 0.5*task_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_explanations = task_predictor.explain(c_emb, c_pred, 'local')\n",
    "global_explanations = task_predictor.explain(c_emb, c_pred, 'global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_explain as te\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "x0 = torch.zeros((4, 100))\n",
    "x_train = torch.tensor([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "], dtype=torch.float)\n",
    "x_train = torch.cat([x_train, x0], dim=1)\n",
    "y_train = torch.tensor([0, 1, 1, 0], dtype=torch.long)\n",
    "y_train_1h = one_hot(y_train).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    te.nn.EntropyLinear(x_train.shape[1], 10, n_classes=y_train_1h.shape[1]),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(10, 4),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(4, 1),\n",
    "]\n",
    "model = torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "loss_form = torch.nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "for epoch in range(2001):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train).squeeze(-1)\n",
    "    loss = loss_form(y_pred, y_train_1h) + 0.0001 * te.nn.functional.entropy_logic_loss(model)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_explain.logic.nn import entropy\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "y1h = one_hot(y_train)\n",
    "global_explanations, local_explanations = entropy.explain_classes(model, x_train, y_train, c_threshold=0.5, y_threshold=0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
